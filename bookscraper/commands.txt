1. Open shell: (scrapy shell)
2. Run spider: (scrapy crawl bookspider)
3. Run & save file spider: (scrapy crawl bookspider -O file_name.file_format) Like: bookdata.json, bookdata.csv
3. Run & save file spider: (scrapy crawl bookspider -o file_name.file_format)

Note:
	--> differece between capital O & small o
		--> O - overright the file from begging
		--> o - start the file from pendding

4. if we add FEEDS in settings, then the seleted format will save and command is.
	--> scrapy crawl bookspider

5. Install mysql & mysql-connector -> pip install mysql mysql-connector-python

6. Fake user agent api: https://scrapeops.io/app/headers

7, Rotate proxy list: 
	--> https://pypi.org/project/scrapy-rotating-proxies/
	--> pip install scrapy-rotating-proxies

	--> https://geonode.com/free-proxy-list
		--> collect proxy ip from here
	--> https://smartproxy.com/

8. For scrapyops proxy solution can also use scrapy package.
	--> pip install scrapeops-scrapy-proxy-sdk