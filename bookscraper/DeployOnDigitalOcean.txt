1. Create a droplet

2. Open terminal
3. sudo apt update
4. sudo apt install python3-pip
5. git clone repository
6. pip install virtualenv
7. virtualenv venv
8. source venv/bin/activate
9. pip install -r requirements.txt
10. To solve mysqlclient error: sudo apt-get install python3-dev default-libmysqlclient-dev build-essential pkg-config
11. Now test setup: cd bookscraper:
	i. scrapy list
		--> it will show list of splider. like: bookspider
12. Now run: scrapy crawl bookspider


--------------DEPLOY BY SCRAPYD---------------
13. pip install scrapyd
14. run it inside bookscraper/bookscraper/
	--> scrapyd > scrapyd_logs.txt 2>&1 &
15. Text scrapyd background running: curl http://localhost:6800/daemonstatus.json
	--> result: {"node_name": "scrapy", "status": "ok", "pending": 0, "running": 0, "finished": 0}
-----------------
Package up our spider and then deploy it to scrapyd. Because if we don't do that, scrapyd lot of access to our project and not able to run the project. so install scrapyd-client.
16. pip install git+https://github.com/scrapy/scrapyd-client.git
-----------------
Now update scrapy.cfg
17. nano scrapy.cfg
	--> remove comment for : url = http://localhost:6800/
18. Deploy scrapyd in scrapy.cfg file directory.
	--> scrapyd-deploy default
19. Now run the spider
	--> $ curl http://localhost:6800/schedule.json -d project=default -d spider=bookspider
		--> result with jobid: {"node_name": "scrapy", "status": "ok", "jobid": "376c82ac4ed011ee801f011a7d27b2cf"}
	--> if not work try, curl http://localhost:6800/schedule.json -d project=bookscraper -d spider=bookspider
		--> {"node_name": "scrapy", "status": "ok", "jobid": "f9e670364ef311ee9f1ed945ed38de07"}
-----------------
For schedule scrapyd instal Scrapydweb: carefull, this does not support greater then 3.9. Better try 3.8
20. pip install --upgrade git+https://github.com/my8100/scrapydweb.git
21. pip install Flask-SQLAlchemy==2.4.0
22. pip install SQLAlchemy
23. pip install Flask
24. pip install Werkzeug

-------------Scrapydweb UI Dashboard---------
25. Now run : scrapydweb
	--> if ok, then get bellow text. also check by ls. got a file named. scrapydweb_settings_v10.py
		-->
		"""
			[2023-09-09 06:01:06,513] INFO     in apscheduler.scheduler: Scheduler started
			[2023-09-09 06:01:06,525] INFO     in scrapydweb.run: ScrapydWeb version: 1.4.1
			[2023-09-09 06:01:06,526] INFO     in scrapydweb.run: Use 'scrapydweb -h' to get help
			[2023-09-09 06:01:06,526] INFO     in scrapydweb.run: Main pid: 22969
			[2023-09-09 06:01:06,526] DEBUG    in scrapydweb.run: Loading default settings from /root/scrapy/venv/lib/python3.8/site-packages/scrapydweb/default_settings.py
			[2023-09-09 06:01:06,639] ERROR    in scrapydweb.run: scrapydweb_settings_v10.py not found: 

			ATTENTION:
			You may encounter ERROR if there are any running timer tasks added in v1.2.0,
			and you have to restart scrapydweb and manually edit the tasks to resume them.

			The config file 'scrapydweb_settings_v10.py' has been copied to current working directory.
			Please add your SCRAPYD_SERVERS in the config file and restart scrapydweb.
		"""
	--> Run again: scrapydweb.
		-->
		"""
			[2023-09-09 06:02:46,286] INFO     in apscheduler.scheduler: Scheduler started
			[2023-09-09 06:02:46,294] INFO     in scrapydweb.run: ScrapydWeb version: 1.4.1
			[2023-09-09 06:02:46,295] INFO     in scrapydweb.run: Use 'scrapydweb -h' to get help
			[2023-09-09 06:02:46,295] INFO     in scrapydweb.run: Main pid: 22976
			[2023-09-09 06:02:46,295] DEBUG    in scrapydweb.run: Loading default settings from /root/scrapy/venv/lib/python3.8/site-packages/scrapydweb/default_settings.py

			****************************************************************************************************
			Overriding custom settings from /root/scrapy/scrapydweb_settings_v10.py
			****************************************************************************************************

			[2023-09-09 06:02:46,368] DEBUG    in scrapydweb.run: Reading settings from command line: Namespace(bind='0.0.0.0', debug=False, disable_auth=False, disable_logparser=False, disable_monitor=False, port=5000, scrapyd_server=None, switch_scheduler_state=False, verbose=False)
			[2023-09-09 06:02:46,368] DEBUG    in scrapydweb.utils.check_app_config: Checking app config
			[2023-09-09 06:02:46,370] INFO     in scrapydweb.utils.check_app_config: Setting up URL_SCRAPYDWEB: http://127.0.0.1:5000
			[2023-09-09 06:02:46,370] DEBUG    in scrapydweb.utils.check_app_config: Checking connectivity of SCRAPYD_SERVERS...
			[2023-09-09 06:02:46,384] ERROR    in scrapydweb.utils.check_app_config: HTTPConnectionPool(host='localhost', port=6801): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f83fa6ad190>: Failed to establish a new connection: [Errno 111] Connection refused'))

			Index Group                Scrapyd IP:Port       Connectivity Auth
			####################################################################################################
			1____ None________________ 127.0.0.1:6800________ True_______ None
			2____ group_______________ localhost:6801________ False______ ('username', 'password')
			####################################################################################################

			/root/scrapy/venv/lib/python3.8/site-packages/sqlalchemy/ext/declarative/clsregistry.py:125: SAWarning: This declarative base already contains a class with the same class name and module name as scrapydweb.models.Job, and will be replaced in the string-lookup table.
			  util.warn(
			[2023-09-09 06:02:46,454] DEBUG    in scrapydweb.utils.check_app_config: Created 2 tables for JobsView
			[2023-09-09 06:02:46,454] INFO     in scrapydweb.utils.check_app_config: Locating scrapy logfiles with SCRAPYD_LOG_EXTENSIONS: ['.log', '.log.gz', '.txt']
			[2023-09-09 06:02:46,458] INFO     in scrapydweb.utils.check_app_config: Scheduler for timer tasks: STATE_RUNNING
			[2023-09-09 06:02:46,465] INFO     in scrapydweb.utils.check_app_config: create_jobs_snapshot (trigger: interval[0:05:00], next run at: 2023-09-09 06:07:46 UTC)

			****************************************************************************************************
			Visit ScrapydWeb at http://127.0.0.1:5000 or http://IP-OF-THE-CURRENT-HOST:5000
			****************************************************************************************************


			[2023-09-09 06:02:46,470] INFO     in scrapydweb.run: For running Flask in production, check out http://flask.pocoo.org/docs/1.0/deploying/
			 * Serving Flask app "scrapydweb" (lazy loading)
			 * Environment: production
			   WARNING: Do not use the development server in a production environment.
			   Use a production WSGI server instead.
			 * Debug mode: off
			[2023-09-09 06:02:46,477] INFO     in werkzeug:  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)
		"""
26. Now go and check scrapy ui dashboard: http://157.230.240.223:5000/1/servers/

27. Now from http://157.230.240.223:5000/1/jobs/ we will get notifation to install logparse.so install it
	--> pip install logparser
28. edit scrapydweb_settings_v10.py file by nano scrapydweb_settings_v10.py.
	--> update:
		--> ENABLE_AUTH = True
		--> USERNAME = 'admin'
		--> PASSWORD = 'adminmilon'
		--> comment SCRAPYD_SERVERS
			--> SCRAPYD_SERVERS
    			--> #('username', 'password', 'localhost', '6801', 'group'),
    	--> LOCAL_SCRAPYD_SERVER = '127.0.0.1:6800'
    	--> also update LOCAL_SCRAPYD_LOGS_DIR. get directory from '/root/scrapy/bookscraper/logs'
    		--> LOCAL_SCRAPYD_LOGS_DIR = '/root/scrapy/logs'
    	--> ENABLE_LOGPARSER = True
29. run : scrapydweb > scrapydweb_logs.txt 2>&1 &

----------------
Now check everything is running curretly
30. sudo ss -tunlp
	--> get bellow.
	"""
		Netid            State             Recv-Q            Send-Q                        Local Address:Port                         Peer Address:Port            Process                                                 
		udp              UNCONN            0                 0                             127.0.0.53%lo:53                                0.0.0.0:*                users:(("systemd-resolve",pid=13205,fd=13))            
		tcp              LISTEN            0                 128                                 0.0.0.0:5000                              0.0.0.0:*                users:(("scrapydweb",pid=23179,fd=4))                  
		tcp              LISTEN            0                 50                                127.0.0.1:6800                              0.0.0.0:*                users:(("scrapyd",pid=10300,fd=6))                     
		tcp              LISTEN            0                 4096                          127.0.0.53%lo:53                                0.0.0.0:*                users:(("systemd-resolve",pid=13205,fd=14))            
		tcp              LISTEN            0                 128                                 0.0.0.0:22                                0.0.0.0:*                users:(("sshd",pid=20709,fd=3))                        
		tcp              LISTEN            0                 128                                    [::]:22                                   [::]:*                users:(("sshd",pid=20709,fd=4)) 
	"""

31. Now to stop any process from the list. like stop scrapyd.
	--> kill pid
		--> kill 10300
32. To start scrapyd again
	--> scrapyd > scrapyd_logs.txt 2>&1 &
33. check running process again
	--> sudo ss -tunlp
---------------
As we killed the process, we have to redeploy.
34. scrapyd-deploy default
Now go and check: http://157.230.240.223:5000/

--------Run the spider in web---------
Go to : http://157.230.240.223:5000/1/schedule/
35. Select project and check CMD and RUN Spider
Go to : http://157.230.240.223:5000/1/jobs/
36. Start the job by Action Start
----------------------
After fixing LOCAL_SCRAPYD_LOGS_DIR in scrapydweb_settings_v10.py. kill and start by following commands
37. sudo ss -tunlp 
38. kill scrapydweb_pid
38. Run scrapydweb > scrapydweb_logs.txt 2>&1 &
Now check http://157.230.240.223:5000/1/jobs/





----------ScrapyOPS UI Dashboard---------
From https://scrapeops.io/app/monitoring-guide follow instruction
39. pip install scrapeops-scrapy
40. Now update settings file.
	--> SCRAPEOPS_API_KEY = '43a93531-f688-4d21-b51a-e339f3e7eaef'
  
	--> EXTENSIONS = {
        	'scrapeops_scrapy.extension.ScrapeOpsMonitor': 500, 
        }
  
	--> DOWNLOADER_MIDDLEWARES = {
        	'scrapeops_scrapy.middleware.retry.RetryMiddleware': 550,
        	'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,
        }
41. Now go to Service & Deployment. https://scrapeops.io/app/servers --> https://scrapeops.io/app/servers/create
	--> From Scrapyd
		--> set server name
		--> server ip address from digital ocean and save
		--> from Server provisioning
			--> simple install (Ubuntu) run bellow command in root dir
				--> wget -O scrapeops_setup.sh "https://assets-scrapeops.nyc3.digitaloceanspaces.com/Bash_Scripts/scrapeops_setup.sh"; bash scrapeops_setup.sh
	--> Now go back and check https://scrapeops.io/app/servers
		--> if Status connected, then connection ok.
	--> From server details, click here and Schedule jobs
		--> if everything ok, then we will get our spider (bookspider) here.
		--> After schedule a job go to -> https://scrapeops.io/app/jobs
	--> Go to Jobs to see the scheduled jobs

42. If there are any issues, debuge in scrapy/bookscraper/
	--> scrapy list
		--> if ok, it will return bookspider

--------------DEPLOY BY SCRAPYD---------------